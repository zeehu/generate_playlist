# generate_playlist: 生成式歌单推荐系统 (v2.0)

(项目介绍、特点、结构等部分与之前相同，此处省略...)

## ⚙️ 环境与安装

(此部分与之前相同...)

## 💿 数据准备

(此部分与之前相同...)

## 🚀 执行步骤

(此部分与之前相同...)

---

## 📊 评估指标解读

在您运行 `evaluate_tiger.py` 脚本后，会在终端看到模型的定量评估结果。以下是各项指标的含义及效果好坏的参考范围，帮助您更好地判断模型表现。

### 1. ROUGE-L (F1-Score on Semantic IDs)

- **指标含义**: 这是我们最关注的指标之一。它通过计算模型生成的“语义ID序列”与真实“语义ID序列”之间的最长公共子序列，来衡量两个序列的重合度。它更侧重于**召回率**，即“真实歌单里的概念，有多少被模型预测出来了”，并且对语序有一定的考虑。

- **效果范围参考**:
    - **< 0.2**: 模型基本没学到序列的结构信息。
    - **0.2 - 0.4**: 模型有了一定的序列生成能力，能预测出部分正确的“概念”，但还有很大提升空间。
    - **0.4 - 0.6**: **一个非常好的结果**。这表明模型准确地捕捉到了大部分核心“概念”以及它们的相对顺序。
    - **> 0.6**: 极好的结果，在学术界和工业界都很有竞争力。

### 2. BLEU-4 Score on Semantic IDs

- **指标含义**: 这是一个源于机器翻译的、非常严格的指标。它计算的是模型生成的序列中，连续4个语义ID组成的“词组”（4-gram）在真实序列中出现的精确度。它非常考验模型生成**局部连贯序列**的能力。

- **效果范围参考**:
    - **≈ 0**: 模型无法生成任何连续正确的“歌曲组合”。
    - **0.05 - 0.15**: **一个不错的起点**。对于我们这种词汇表巨大（`vocab_size=2048`）、且带有创造性的生成任务，获得一个显著非零的BLEU-4分数，已经证明了模型学会了生成一些有意义的“局部乐句”。
    - **0.15 - 0.30**: **非常好的结果**。说明模型能生成大量在真实歌单中出现过的、连贯的“歌曲组合”。
    - **> 0.30**: 极好的结果，通常很难达到。

### (备用) Avg. Song F1-Score (歌曲集合F1分数)

- **指标含义**: 这是我们之前“簇扩展”评估方案中使用的指标。它完全忽略顺序，将被预测的所有“概念簇”展开为一个大的歌曲集合，然后与真实歌单的歌曲集合计算F1分数。它最能直观地反映**最终推荐结果的准确性**。

- **效果范围参考**:
    - **< 0.1**: 模型推荐的歌曲与真实歌单重合度很低。
    - **0.1 - 0.3**: 有效的基线模型，已经能推荐出部分相关歌曲。
    - **0.3 - 0.5**: **一个非常好的结果**。表明模型的推荐兼具了不错的准确率和召回率。
    - **> 0.5**: 极好的结果，在业界也很有竞争力。

**总结**: 在当前的评估脚本中，我们主要关注 **ROUGE-L** 和 **BLEU-4**。如果您获得了例如 `ROUGE-L > 0.4` 且 `BLEU-4 > 0.1` 的结果，就可以认为您已经拥有了一个相当不错的、可以投入“定性分析和演示”的模型。